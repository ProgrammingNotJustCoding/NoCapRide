[
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "sync_playwright",
        "importPath": "playwright.sync_api",
        "description": "playwright.sync_api",
        "isExtraImport": true,
        "detail": "playwright.sync_api",
        "documentation": {}
    },
    {
        "label": "schedule",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "schedule",
        "description": "schedule",
        "detail": "schedule",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "GradientBoostingRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "r2_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "r2_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "OneHotEncoder",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "ColumnTransformer",
        "importPath": "sklearn.compose",
        "description": "sklearn.compose",
        "isExtraImport": true,
        "detail": "sklearn.compose",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "sklearn.pipeline",
        "description": "sklearn.pipeline",
        "isExtraImport": true,
        "detail": "sklearn.pipeline",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "holidays",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "holidays",
        "description": "holidays",
        "detail": "holidays",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "signal",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "signal",
        "description": "signal",
        "detail": "signal",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "load_and_preprocess_data",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "create_features",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "predict_earnings",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "load_and_preprocess_data",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "create_features",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "train_models",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "CORSMiddleware",
        "importPath": "fastapi.middleware.cors",
        "description": "fastapi.middleware.cors",
        "isExtraImport": true,
        "detail": "fastapi.middleware.cors",
        "documentation": {}
    },
    {
        "label": "CORSMiddleware",
        "importPath": "fastapi.middleware.cors",
        "description": "fastapi.middleware.cors",
        "isExtraImport": true,
        "detail": "fastapi.middleware.cors",
        "documentation": {}
    },
    {
        "label": "Request",
        "importPath": "starlette.requests",
        "description": "starlette.requests",
        "isExtraImport": true,
        "detail": "starlette.requests",
        "documentation": {}
    },
    {
        "label": "Request",
        "importPath": "starlette.requests",
        "description": "starlette.requests",
        "isExtraImport": true,
        "detail": "starlette.requests",
        "documentation": {}
    },
    {
        "label": "RideDataManager",
        "importPath": "panda",
        "description": "panda",
        "isExtraImport": true,
        "detail": "panda",
        "documentation": {}
    },
    {
        "label": "RideRequestForecast",
        "importPath": "panda",
        "description": "panda",
        "isExtraImport": true,
        "detail": "panda",
        "documentation": {}
    },
    {
        "label": "RideDataManager",
        "importPath": "panda",
        "description": "panda",
        "isExtraImport": true,
        "detail": "panda",
        "documentation": {}
    },
    {
        "label": "RideRequestForecast",
        "importPath": "panda",
        "description": "panda",
        "isExtraImport": true,
        "detail": "panda",
        "documentation": {}
    },
    {
        "label": "run_simple",
        "importPath": "werkzeug.serving",
        "description": "werkzeug.serving",
        "isExtraImport": true,
        "detail": "werkzeug.serving",
        "documentation": {}
    },
    {
        "label": "concurrent.futures",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "_Known",
        "kind": 6,
        "importPath": "client.node_modules..pnpm.flat-cache@4.0.1.node_modules.flatted.python.flatted",
        "description": "client.node_modules..pnpm.flat-cache@4.0.1.node_modules.flatted.python.flatted",
        "peekOfCode": "class _Known:\n    def __init__(self):\n        self.key = []\n        self.value = []\nclass _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0",
        "detail": "client.node_modules..pnpm.flat-cache@4.0.1.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "_String",
        "kind": 6,
        "importPath": "client.node_modules..pnpm.flat-cache@4.0.1.node_modules.flatted.python.flatted",
        "description": "client.node_modules..pnpm.flat-cache@4.0.1.node_modules.flatted.python.flatted",
        "peekOfCode": "class _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0\n    for _ in value:\n        keys.append(i)\n        i += 1\n    return keys",
        "detail": "client.node_modules..pnpm.flat-cache@4.0.1.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "parse",
        "kind": 2,
        "importPath": "client.node_modules..pnpm.flat-cache@4.0.1.node_modules.flatted.python.flatted",
        "description": "client.node_modules..pnpm.flat-cache@4.0.1.node_modules.flatted.python.flatted",
        "peekOfCode": "def parse(value, *args, **kwargs):\n    json = _json.loads(value, *args, **kwargs)\n    wrapped = []\n    for value in json:\n        wrapped.append(_wrap(value))\n    input = []\n    for value in wrapped:\n        if isinstance(value, _String):\n            input.append(value.value)\n        else:",
        "detail": "client.node_modules..pnpm.flat-cache@4.0.1.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "stringify",
        "kind": 2,
        "importPath": "client.node_modules..pnpm.flat-cache@4.0.1.node_modules.flatted.python.flatted",
        "description": "client.node_modules..pnpm.flat-cache@4.0.1.node_modules.flatted.python.flatted",
        "peekOfCode": "def stringify(value, *args, **kwargs):\n    known = _Known()\n    input = []\n    output = []\n    i = int(_index(known, input, value))\n    while i < len(input):\n        output.append(_transform(known, input, input[i]))\n        i += 1\n    return _json.dumps(output, *args, **kwargs)",
        "detail": "client.node_modules..pnpm.flat-cache@4.0.1.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "_Known",
        "kind": 6,
        "importPath": "client.node_modules..pnpm.flatted@3.3.3.node_modules.flatted.python.flatted",
        "description": "client.node_modules..pnpm.flatted@3.3.3.node_modules.flatted.python.flatted",
        "peekOfCode": "class _Known:\n    def __init__(self):\n        self.key = []\n        self.value = []\nclass _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0",
        "detail": "client.node_modules..pnpm.flatted@3.3.3.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "_String",
        "kind": 6,
        "importPath": "client.node_modules..pnpm.flatted@3.3.3.node_modules.flatted.python.flatted",
        "description": "client.node_modules..pnpm.flatted@3.3.3.node_modules.flatted.python.flatted",
        "peekOfCode": "class _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0\n    for _ in value:\n        keys.append(i)\n        i += 1\n    return keys",
        "detail": "client.node_modules..pnpm.flatted@3.3.3.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "parse",
        "kind": 2,
        "importPath": "client.node_modules..pnpm.flatted@3.3.3.node_modules.flatted.python.flatted",
        "description": "client.node_modules..pnpm.flatted@3.3.3.node_modules.flatted.python.flatted",
        "peekOfCode": "def parse(value, *args, **kwargs):\n    json = _json.loads(value, *args, **kwargs)\n    wrapped = []\n    for value in json:\n        wrapped.append(_wrap(value))\n    input = []\n    for value in wrapped:\n        if isinstance(value, _String):\n            input.append(value.value)\n        else:",
        "detail": "client.node_modules..pnpm.flatted@3.3.3.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "stringify",
        "kind": 2,
        "importPath": "client.node_modules..pnpm.flatted@3.3.3.node_modules.flatted.python.flatted",
        "description": "client.node_modules..pnpm.flatted@3.3.3.node_modules.flatted.python.flatted",
        "peekOfCode": "def stringify(value, *args, **kwargs):\n    known = _Known()\n    input = []\n    output = []\n    i = int(_index(known, input, value))\n    while i < len(input):\n        output.append(_transform(known, input, input[i]))\n        i += 1\n    return _json.dumps(output, *args, **kwargs)",
        "detail": "client.node_modules..pnpm.flatted@3.3.3.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "_Known",
        "kind": 6,
        "importPath": "client.node_modules..pnpm.node_modules.flatted.python.flatted",
        "description": "client.node_modules..pnpm.node_modules.flatted.python.flatted",
        "peekOfCode": "class _Known:\n    def __init__(self):\n        self.key = []\n        self.value = []\nclass _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0",
        "detail": "client.node_modules..pnpm.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "_String",
        "kind": 6,
        "importPath": "client.node_modules..pnpm.node_modules.flatted.python.flatted",
        "description": "client.node_modules..pnpm.node_modules.flatted.python.flatted",
        "peekOfCode": "class _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0\n    for _ in value:\n        keys.append(i)\n        i += 1\n    return keys",
        "detail": "client.node_modules..pnpm.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "parse",
        "kind": 2,
        "importPath": "client.node_modules..pnpm.node_modules.flatted.python.flatted",
        "description": "client.node_modules..pnpm.node_modules.flatted.python.flatted",
        "peekOfCode": "def parse(value, *args, **kwargs):\n    json = _json.loads(value, *args, **kwargs)\n    wrapped = []\n    for value in json:\n        wrapped.append(_wrap(value))\n    input = []\n    for value in wrapped:\n        if isinstance(value, _String):\n            input.append(value.value)\n        else:",
        "detail": "client.node_modules..pnpm.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "stringify",
        "kind": 2,
        "importPath": "client.node_modules..pnpm.node_modules.flatted.python.flatted",
        "description": "client.node_modules..pnpm.node_modules.flatted.python.flatted",
        "peekOfCode": "def stringify(value, *args, **kwargs):\n    known = _Known()\n    input = []\n    output = []\n    i = int(_index(known, input, value))\n    while i < len(input):\n        output.append(_transform(known, input, input[i]))\n        i += 1\n    return _json.dumps(output, *args, **kwargs)",
        "detail": "client.node_modules..pnpm.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "capture_endpoints",
        "kind": 2,
        "importPath": "models.fetch",
        "description": "models.fetch",
        "peekOfCode": "def capture_endpoints(url):\n    \"\"\"Capture all API endpoints from the webpage\"\"\"\n    logger.info(f\"Capturing endpoints from {url}\")\n    try:\n        with sync_playwright() as p:\n            browser = p.chromium.launch(headless=True)\n            context = browser.new_context()\n            page = context.new_page()\n            endpoints = []\n            def log_request(request):",
        "detail": "models.fetch",
        "documentation": {}
    },
    {
        "label": "fetch_and_save",
        "kind": 2,
        "importPath": "models.fetch",
        "description": "models.fetch",
        "peekOfCode": "def fetch_and_save(endpoint):\n    \"\"\"Fetch data from an endpoint and save it to a file\"\"\"\n    try:\n        # Perform the fetch request\n        logger.info(f\"Fetching data from {endpoint}\")\n        response = requests.get(endpoint, timeout=30)  # Add timeout\n        # Check if the request was successful\n        if response.status_code == 200:\n            # Extract the endpoint name for the file name\n            endpoint_name = endpoint.split(\"/\")[-1]",
        "detail": "models.fetch",
        "documentation": {}
    },
    {
        "label": "initialize_endpoints",
        "kind": 2,
        "importPath": "models.fetch",
        "description": "models.fetch",
        "peekOfCode": "def initialize_endpoints():\n    \"\"\"Initialize by discovering all endpoints and saving them\"\"\"\n    global discovered_endpoints\n    try:\n        # Check if we already have saved endpoints\n        if os.path.exists(ENDPOINTS_FILE):\n            with open(ENDPOINTS_FILE, \"r\") as f:\n                discovered_endpoints = json.load(f)\n            logger.info(f\"Loaded {len(discovered_endpoints)} endpoints from file\")\n        else:",
        "detail": "models.fetch",
        "documentation": {}
    },
    {
        "label": "fetch_all_endpoints",
        "kind": 2,
        "importPath": "models.fetch",
        "description": "models.fetch",
        "peekOfCode": "def fetch_all_endpoints():\n    \"\"\"Fetch data from all known endpoints\"\"\"\n    try:\n        logger.info(f\"Fetching data from {len(discovered_endpoints)} endpoints\")\n        success_count = 0\n        for endpoint in discovered_endpoints:\n            if fetch_and_save(endpoint):\n                success_count += 1\n        logger.info(\n            f\"Successfully fetched {success_count}/{len(discovered_endpoints)} endpoints\"",
        "detail": "models.fetch",
        "documentation": {}
    },
    {
        "label": "create_sample_data",
        "kind": 2,
        "importPath": "models.fetch",
        "description": "models.fetch",
        "peekOfCode": "def create_sample_data():\n    \"\"\"Create sample data files for testing\"\"\"\n    try:\n        logger.info(\"Creating sample data files\")\n        # Get unique ward numbers from existing data files\n        ward_nums = set()\n        try:\n            with open(\"data/driver_eda_wards_new_key.json\", \"r\") as f:\n                driver_data = json.load(f)\n                for entry in driver_data:",
        "detail": "models.fetch",
        "documentation": {}
    },
    {
        "label": "scheduled_fetch",
        "kind": 2,
        "importPath": "models.fetch",
        "description": "models.fetch",
        "peekOfCode": "def scheduled_fetch():\n    \"\"\"Function to be called by the scheduler\"\"\"\n    try:\n        logger.info(\"Running scheduled fetch\")\n        success = fetch_all_endpoints()\n        if success:\n            logger.info(\"Scheduled fetch completed successfully\")\n        else:\n            logger.error(\"Scheduled fetch failed\")\n    except Exception as e:",
        "detail": "models.fetch",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "models.fetch",
        "description": "models.fetch",
        "peekOfCode": "def main():\n    \"\"\"Main function to run the continuous fetching process\"\"\"\n    try:\n        logger.info(\"Starting fetch service\")\n        # Initialize endpoints\n        if not initialize_endpoints():\n            logger.error(\"Failed to initialize endpoints, exiting\")\n            sys.exit(1)\n        # Schedule fetching every 2 minutes\n        schedule.every(2).minutes.do(scheduled_fetch)",
        "detail": "models.fetch",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "models.fetch",
        "description": "models.fetch",
        "peekOfCode": "logger = logging.getLogger(\"fetch\")\n# Create a separate handler for detailed output\ndetailed_logger = logging.getLogger(\"fetch_detailed\")\ndetailed_logger.setLevel(logging.INFO)\ndetailed_handler = logging.FileHandler(\"logs/fetch_output.log\", mode=\"a\")\ndetailed_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\"))\ndetailed_logger.addHandler(detailed_handler)\ndetailed_logger.propagate = False  # Prevent double logging\n# Ensure the /data directory exists\nif not os.path.exists(\"data\"):",
        "detail": "models.fetch",
        "documentation": {}
    },
    {
        "label": "detailed_logger",
        "kind": 5,
        "importPath": "models.fetch",
        "description": "models.fetch",
        "peekOfCode": "detailed_logger = logging.getLogger(\"fetch_detailed\")\ndetailed_logger.setLevel(logging.INFO)\ndetailed_handler = logging.FileHandler(\"logs/fetch_output.log\", mode=\"a\")\ndetailed_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\"))\ndetailed_logger.addHandler(detailed_handler)\ndetailed_logger.propagate = False  # Prevent double logging\n# Ensure the /data directory exists\nif not os.path.exists(\"data\"):\n    os.makedirs(\"data\")\n# Store discovered endpoints",
        "detail": "models.fetch",
        "documentation": {}
    },
    {
        "label": "detailed_handler",
        "kind": 5,
        "importPath": "models.fetch",
        "description": "models.fetch",
        "peekOfCode": "detailed_handler = logging.FileHandler(\"logs/fetch_output.log\", mode=\"a\")\ndetailed_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\"))\ndetailed_logger.addHandler(detailed_handler)\ndetailed_logger.propagate = False  # Prevent double logging\n# Ensure the /data directory exists\nif not os.path.exists(\"data\"):\n    os.makedirs(\"data\")\n# Store discovered endpoints\nENDPOINTS_FILE = \"data/endpoints.json\"\ndiscovered_endpoints = []",
        "detail": "models.fetch",
        "documentation": {}
    },
    {
        "label": "detailed_logger.propagate",
        "kind": 5,
        "importPath": "models.fetch",
        "description": "models.fetch",
        "peekOfCode": "detailed_logger.propagate = False  # Prevent double logging\n# Ensure the /data directory exists\nif not os.path.exists(\"data\"):\n    os.makedirs(\"data\")\n# Store discovered endpoints\nENDPOINTS_FILE = \"data/endpoints.json\"\ndiscovered_endpoints = []\ndef capture_endpoints(url):\n    \"\"\"Capture all API endpoints from the webpage\"\"\"\n    logger.info(f\"Capturing endpoints from {url}\")",
        "detail": "models.fetch",
        "documentation": {}
    },
    {
        "label": "ENDPOINTS_FILE",
        "kind": 5,
        "importPath": "models.fetch",
        "description": "models.fetch",
        "peekOfCode": "ENDPOINTS_FILE = \"data/endpoints.json\"\ndiscovered_endpoints = []\ndef capture_endpoints(url):\n    \"\"\"Capture all API endpoints from the webpage\"\"\"\n    logger.info(f\"Capturing endpoints from {url}\")\n    try:\n        with sync_playwright() as p:\n            browser = p.chromium.launch(headless=True)\n            context = browser.new_context()\n            page = context.new_page()",
        "detail": "models.fetch",
        "documentation": {}
    },
    {
        "label": "discovered_endpoints",
        "kind": 5,
        "importPath": "models.fetch",
        "description": "models.fetch",
        "peekOfCode": "discovered_endpoints = []\ndef capture_endpoints(url):\n    \"\"\"Capture all API endpoints from the webpage\"\"\"\n    logger.info(f\"Capturing endpoints from {url}\")\n    try:\n        with sync_playwright() as p:\n            browser = p.chromium.launch(headless=True)\n            context = browser.new_context()\n            page = context.new_page()\n            endpoints = []",
        "detail": "models.fetch",
        "documentation": {}
    },
    {
        "label": "load_and_preprocess_data",
        "kind": 2,
        "importPath": "models.model",
        "description": "models.model",
        "peekOfCode": "def load_and_preprocess_data(trends_file, driver_file, funnel_file=None):\n    \"\"\"\n    Load and preprocess data from the given files\n    Args:\n        trends_file: Path to the trends data file\n        driver_file: Path to the driver data file\n        funnel_file: Path to the funnel data file (optional)\n    Returns:\n        Preprocessed DataFrame with all features\n    \"\"\"",
        "detail": "models.model",
        "documentation": {}
    },
    {
        "label": "create_features",
        "kind": 2,
        "importPath": "models.model",
        "description": "models.model",
        "peekOfCode": "def create_features(df):\n    \"\"\"\n    Create features for model training\n    Args:\n        df: DataFrame with preprocessed data\n    Returns:\n        DataFrame with features for model training\n    \"\"\"\n    try:\n        df = df.copy()",
        "detail": "models.model",
        "documentation": {}
    },
    {
        "label": "train_models",
        "kind": 2,
        "importPath": "models.model",
        "description": "models.model",
        "peekOfCode": "def train_models(df, target_variable=\"avg_earning_per_ride\"):\n    \"\"\"\n    Train multiple models on the given data\n    Args:\n        df: DataFrame with features\n        target_variable: Target variable to predict\n    Returns:\n        Dictionary of trained models and their features\n    \"\"\"\n    try:",
        "detail": "models.model",
        "documentation": {}
    },
    {
        "label": "predict_earnings",
        "kind": 2,
        "importPath": "models.model",
        "description": "models.model",
        "peekOfCode": "def predict_earnings(models, input_data, model_name=None):\n    \"\"\"\n    Make predictions using the trained models\n    Args:\n        models: Dictionary of trained models from train_models\n        input_data: DataFrame with input features\n        model_name: Name of the model to use for prediction (if None, use the best model)\n    Returns:\n        DataFrame with predictions\n    \"\"\"",
        "detail": "models.model",
        "documentation": {}
    },
    {
        "label": "load_and_preprocess_data",
        "kind": 2,
        "importPath": "models.old-demand-area",
        "description": "models.old-demand-area",
        "peekOfCode": "def load_and_preprocess_data(trends_file, driver_file, funnel_file=None):\n    try:\n        trends_df = pd.read_json(trends_file)\n        driver_df = pd.read_json(driver_file)\n        print(\"\\nData Loading Diagnostics:\")\n        print(\"Trends DataFrame columns:\", trends_df.columns)\n        print(\"Driver DataFrame columns:\", driver_df.columns)\n        print(\"\\nTrends DataFrame sample:\")\n        print(trends_df.head())\n        print(\"\\nDriver DataFrame sample:\")",
        "detail": "models.old-demand-area",
        "documentation": {}
    },
    {
        "label": "create_features",
        "kind": 2,
        "importPath": "models.old-demand-area",
        "description": "models.old-demand-area",
        "peekOfCode": "def create_features(df):\n    df = df.copy()\n    new_features = {}\n    new_features[\"date\"] = pd.to_datetime(df[\"date\"])\n    new_features[\"hour\"] = df[\"hour\"].astype(int)\n    new_features[\"day_of_week\"] = new_features[\"date\"].dt.dayofweek\n    new_features[\"day_of_month\"] = new_features[\"date\"].dt.day\n    new_features[\"month\"] = new_features[\"date\"].dt.month\n    new_features[\"is_weekend\"] = new_features[\"day_of_week\"].apply(\n        lambda x: 1 if x >= 5 else 0",
        "detail": "models.old-demand-area",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": "models.old-demand-area",
        "description": "models.old-demand-area",
        "peekOfCode": "def train_model(df, target_variable=\"demand_supply_ratio\"):\n    try:\n        exclude_cols = [\n            \"date\",\n            target_variable,\n            \"booking\",\n            \"done_ride\",\n            \"earning\",\n            \"cancel_ride\",\n            \"drvr_cancel\",",
        "detail": "models.old-demand-area",
        "documentation": {}
    },
    {
        "label": "predict_and_recommend",
        "kind": 2,
        "importPath": "models.old-demand-area",
        "description": "models.old-demand-area",
        "peekOfCode": "def predict_and_recommend(\n    model,\n    features,\n    user_time,\n    user_location,\n    user_vehicle_type,\n    all_wards,\n    df,\n    original_df,\n):",
        "detail": "models.old-demand-area",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "models.old-demand-area",
        "description": "models.old-demand-area",
        "peekOfCode": "def main(\n    trends_file, driver_file, funnel_file, user_time, user_location, user_vehicle_type\n):\n    try:\n        merged_df = load_and_preprocess_data(trends_file, driver_file, funnel_file)\n        df_with_features = create_features(merged_df)\n        print(\"\\nData Overview:\")\n        print(\n            f\"Date range: {df_with_features['date'].min()} to {df_with_features['date'].max()}\"\n        )",
        "detail": "models.old-demand-area",
        "documentation": {}
    },
    {
        "label": "check_dependencies",
        "kind": 2,
        "importPath": "models.run",
        "description": "models.run",
        "peekOfCode": "def check_dependencies():\n    \"\"\"Check if all required dependencies are installed\"\"\"\n    try:\n        # Check Python packages\n        required_packages = [\n            \"pandas\",\n            \"numpy\",\n            \"sklearn\",\n            \"flask\",\n            \"schedule\",",
        "detail": "models.run",
        "documentation": {}
    },
    {
        "label": "start_process",
        "kind": 2,
        "importPath": "models.run",
        "description": "models.run",
        "peekOfCode": "def start_process(name, command):\n    \"\"\"Start a subprocess and return the process object\"\"\"\n    try:\n        logger.info(f\"Starting {name} process: {command}\")\n        # Create log file\n        log_file = open(f\"{name}_output.log\", \"w\")\n        # Start the process\n        process = subprocess.Popen(\n            command,\n            stdout=log_file,",
        "detail": "models.run",
        "documentation": {}
    },
    {
        "label": "stop_process",
        "kind": 2,
        "importPath": "models.run",
        "description": "models.run",
        "peekOfCode": "def stop_process(name):\n    \"\"\"Stop a running process\"\"\"\n    if name in processes:\n        process_info = processes[name]\n        process = process_info[\"process\"]\n        try:\n            logger.info(f\"Stopping {name} process (PID {process.pid})\")\n            # Send SIGTERM to the process group\n            os.killpg(os.getpgid(process.pid), signal.SIGTERM)\n            # Wait for process to terminate",
        "detail": "models.run",
        "documentation": {}
    },
    {
        "label": "check_process",
        "kind": 2,
        "importPath": "models.run",
        "description": "models.run",
        "peekOfCode": "def check_process(name):\n    \"\"\"Check if a process is still running and restart if needed\"\"\"\n    if name in processes:\n        process_info = processes[name]\n        process = process_info[\"process\"]\n        # Check if process is still running\n        if process.poll() is not None:\n            # Process has terminated\n            exit_code = process.returncode\n            logger.warning(f\"{name} process has terminated with code {exit_code}\")",
        "detail": "models.run",
        "documentation": {}
    },
    {
        "label": "start_all",
        "kind": 2,
        "importPath": "models.run",
        "description": "models.run",
        "peekOfCode": "def start_all():\n    \"\"\"Start all required processes\"\"\"\n    try:\n        # Start fetch process\n        start_process(\"fetch\", \"python fetch.py\")\n        # Wait for initial data to be fetched\n        logger.info(\"Waiting for initial data to be fetched...\")\n        time.sleep(10)\n        # Check if fetch process is still running\n        if \"fetch\" not in processes:",
        "detail": "models.run",
        "documentation": {}
    },
    {
        "label": "stop_all",
        "kind": 2,
        "importPath": "models.run",
        "description": "models.run",
        "peekOfCode": "def stop_all():\n    \"\"\"Stop all running processes\"\"\"\n    for name in list(processes.keys()):\n        stop_process(name)\n    logger.info(\"All processes stopped\")\ndef monitor_processes():\n    \"\"\"Monitor all processes and restart if needed\"\"\"\n    while True:\n        try:\n            for name in list(processes.keys()):",
        "detail": "models.run",
        "documentation": {}
    },
    {
        "label": "monitor_processes",
        "kind": 2,
        "importPath": "models.run",
        "description": "models.run",
        "peekOfCode": "def monitor_processes():\n    \"\"\"Monitor all processes and restart if needed\"\"\"\n    while True:\n        try:\n            for name in list(processes.keys()):\n                check_process(name)\n            # If all processes have failed too many times, exit\n            if all(count > MAX_RESTART_ATTEMPTS for count in restart_counts.values()):\n                logger.error(\"All processes have failed too many times, exiting\")\n                return",
        "detail": "models.run",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "models.run",
        "description": "models.run",
        "peekOfCode": "def parse_args():\n    \"\"\"Parse command line arguments\"\"\"\n    parser = argparse.ArgumentParser(description=\"Run the demand prediction system\")\n    parser.add_argument(\n        \"--fetch-only\", action=\"store_true\", help=\"Run only the fetch process\"\n    )\n    parser.add_argument(\n        \"--train-only\", action=\"store_true\", help=\"Run only the train process\"\n    )\n    parser.add_argument(",
        "detail": "models.run",
        "documentation": {}
    },
    {
        "label": "reset_data",
        "kind": 2,
        "importPath": "models.run",
        "description": "models.run",
        "peekOfCode": "def reset_data():\n    \"\"\"Reset all data and models\"\"\"\n    try:\n        logger.info(\"Resetting all data and models\")\n        # Remove data directory\n        if os.path.exists(\"data\"):\n            shutil.rmtree(\"data\")\n            logger.info(\"Removed data directory\")\n        # Remove models directory\n        if os.path.exists(\"models\"):",
        "detail": "models.run",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "models.run",
        "description": "models.run",
        "peekOfCode": "def main():\n    \"\"\"Main function\"\"\"\n    args = parse_args()\n    try:\n        logger.info(\"Starting demand prediction system\")\n        # Check dependencies\n        if not check_dependencies():\n            logger.error(\"Dependency check failed, exiting\")\n            return\n        # Reset data if requested",
        "detail": "models.run",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "models.run",
        "description": "models.run",
        "peekOfCode": "logger = logging.getLogger(\"run\")\n# Global variables\nprocesses = {}\nMAX_RESTART_ATTEMPTS = 3\nrestart_counts = {}\ndef check_dependencies():\n    \"\"\"Check if all required dependencies are installed\"\"\"\n    try:\n        # Check Python packages\n        required_packages = [",
        "detail": "models.run",
        "documentation": {}
    },
    {
        "label": "processes",
        "kind": 5,
        "importPath": "models.run",
        "description": "models.run",
        "peekOfCode": "processes = {}\nMAX_RESTART_ATTEMPTS = 3\nrestart_counts = {}\ndef check_dependencies():\n    \"\"\"Check if all required dependencies are installed\"\"\"\n    try:\n        # Check Python packages\n        required_packages = [\n            \"pandas\",\n            \"numpy\",",
        "detail": "models.run",
        "documentation": {}
    },
    {
        "label": "MAX_RESTART_ATTEMPTS",
        "kind": 5,
        "importPath": "models.run",
        "description": "models.run",
        "peekOfCode": "MAX_RESTART_ATTEMPTS = 3\nrestart_counts = {}\ndef check_dependencies():\n    \"\"\"Check if all required dependencies are installed\"\"\"\n    try:\n        # Check Python packages\n        required_packages = [\n            \"pandas\",\n            \"numpy\",\n            \"sklearn\",",
        "detail": "models.run",
        "documentation": {}
    },
    {
        "label": "restart_counts",
        "kind": 5,
        "importPath": "models.run",
        "description": "models.run",
        "peekOfCode": "restart_counts = {}\ndef check_dependencies():\n    \"\"\"Check if all required dependencies are installed\"\"\"\n    try:\n        # Check Python packages\n        required_packages = [\n            \"pandas\",\n            \"numpy\",\n            \"sklearn\",\n            \"flask\",",
        "detail": "models.run",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "def load_model():\n    \"\"\"Load the trained model from disk\"\"\"\n    global models, model_info\n    try:\n        if os.path.exists(MODEL_FILE) and os.path.exists(MODEL_INFO_FILE):\n            with open(MODEL_FILE, \"rb\") as f:\n                models = pickle.load(f)\n            with open(MODEL_INFO_FILE, \"r\") as f:\n                model_info = json.load(f)\n            logger.info(",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "def load_data():\n    \"\"\"Load and preprocess the latest data\"\"\"\n    global data\n    try:\n        # Find the latest data files\n        data_dir = \"data\"\n        trends_file = None\n        driver_file = None\n        funnel_file = None\n        for file in os.listdir(data_dir):",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "data_refresh_thread",
        "kind": 2,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "def data_refresh_thread():\n    \"\"\"Thread function to periodically refresh the data\"\"\"\n    while True:\n        try:\n            load_data()\n            time.sleep(DATA_REFRESH_INTERVAL)\n        except Exception as e:\n            logger.error(f\"Error in data refresh thread: {e}\")\n            logger.error(traceback.format_exc())\n            time.sleep(10)  # Wait a bit before retrying",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "get_ward_recommendations",
        "kind": 2,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "def get_ward_recommendations(user_time, user_location, user_vehicle_type):\n    \"\"\"Get recommendations for the best wards based on earnings potential\"\"\"\n    global models, data\n    try:\n        if models is None or data is None:\n            return {\"error\": \"Model or data not loaded\"}, 500\n        # Get the merged data and feature data\n        merged_df = data[\"merged_df\"]\n        feature_df = data[\"feature_df\"]\n        # Convert user_time to datetime",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "def predict():\n    \"\"\"API endpoint to get predictions\"\"\"\n    try:\n        # Get request data\n        request_data = request.get_json()\n        # Check required parameters\n        if not request_data:\n            return jsonify({\"error\": \"No data provided\"}), 400\n        # Get parameters\n        user_time = request_data.get(\"user_time\", datetime.now().isoformat())",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "health",
        "kind": 2,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "def health():\n    \"\"\"API endpoint to check the health of the service\"\"\"\n    global models, data\n    status = {\n        \"status\": \"healthy\",\n        \"model_loaded\": models is not None,\n        \"data_loaded\": data is not None,\n    }\n    if models is not None and \"best_model\" in models:\n        status[\"best_model\"] = models[\"best_model\"]",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "def main():\n    \"\"\"Main function to start the API server\"\"\"\n    logger.info(\"Starting API server\")\n    # Load the model\n    if not load_model():\n        logger.error(\"Failed to load model, exiting\")\n        sys.exit(1)\n    # Load initial data\n    if not load_data():\n        logger.warning(\"Failed to load initial data, will retry in background\")",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "detailed_logger",
        "kind": 5,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "detailed_logger = logging.getLogger(\"train_detailed\")\ndetailed_logger.setLevel(logging.INFO)\ndetailed_handler = logging.FileHandler(\"logs/serve_output.log\", mode=\"a\")\ndetailed_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\"))\ndetailed_logger.addHandler(detailed_handler)\ndetailed_logger.propagate = False  # Prevent double logging\nlogger = logging.getLogger(\"serve\")\n# Global variables\nMODEL_FILE = \"models/demand_model.pkl\"\nMODEL_INFO_FILE = \"models/model_info.json\"",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "detailed_handler",
        "kind": 5,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "detailed_handler = logging.FileHandler(\"logs/serve_output.log\", mode=\"a\")\ndetailed_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\"))\ndetailed_logger.addHandler(detailed_handler)\ndetailed_logger.propagate = False  # Prevent double logging\nlogger = logging.getLogger(\"serve\")\n# Global variables\nMODEL_FILE = \"models/demand_model.pkl\"\nMODEL_INFO_FILE = \"models/model_info.json\"\nDATA_REFRESH_INTERVAL = 60  # seconds\n# Global data and model objects",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "detailed_logger.propagate",
        "kind": 5,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "detailed_logger.propagate = False  # Prevent double logging\nlogger = logging.getLogger(\"serve\")\n# Global variables\nMODEL_FILE = \"models/demand_model.pkl\"\nMODEL_INFO_FILE = \"models/model_info.json\"\nDATA_REFRESH_INTERVAL = 60  # seconds\n# Global data and model objects\nmodels = None\nmodel_info = None\ndata = None",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "logger = logging.getLogger(\"serve\")\n# Global variables\nMODEL_FILE = \"models/demand_model.pkl\"\nMODEL_INFO_FILE = \"models/model_info.json\"\nDATA_REFRESH_INTERVAL = 60  # seconds\n# Global data and model objects\nmodels = None\nmodel_info = None\ndata = None\ndata_lock = threading.Lock()",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "MODEL_FILE",
        "kind": 5,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "MODEL_FILE = \"models/demand_model.pkl\"\nMODEL_INFO_FILE = \"models/model_info.json\"\nDATA_REFRESH_INTERVAL = 60  # seconds\n# Global data and model objects\nmodels = None\nmodel_info = None\ndata = None\ndata_lock = threading.Lock()\napp = Flask(__name__)\ndef load_model():",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "MODEL_INFO_FILE",
        "kind": 5,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "MODEL_INFO_FILE = \"models/model_info.json\"\nDATA_REFRESH_INTERVAL = 60  # seconds\n# Global data and model objects\nmodels = None\nmodel_info = None\ndata = None\ndata_lock = threading.Lock()\napp = Flask(__name__)\ndef load_model():\n    \"\"\"Load the trained model from disk\"\"\"",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "DATA_REFRESH_INTERVAL",
        "kind": 5,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "DATA_REFRESH_INTERVAL = 60  # seconds\n# Global data and model objects\nmodels = None\nmodel_info = None\ndata = None\ndata_lock = threading.Lock()\napp = Flask(__name__)\ndef load_model():\n    \"\"\"Load the trained model from disk\"\"\"\n    global models, model_info",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "models",
        "kind": 5,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "models = None\nmodel_info = None\ndata = None\ndata_lock = threading.Lock()\napp = Flask(__name__)\ndef load_model():\n    \"\"\"Load the trained model from disk\"\"\"\n    global models, model_info\n    try:\n        if os.path.exists(MODEL_FILE) and os.path.exists(MODEL_INFO_FILE):",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "model_info",
        "kind": 5,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "model_info = None\ndata = None\ndata_lock = threading.Lock()\napp = Flask(__name__)\ndef load_model():\n    \"\"\"Load the trained model from disk\"\"\"\n    global models, model_info\n    try:\n        if os.path.exists(MODEL_FILE) and os.path.exists(MODEL_INFO_FILE):\n            with open(MODEL_FILE, \"rb\") as f:",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "data = None\ndata_lock = threading.Lock()\napp = Flask(__name__)\ndef load_model():\n    \"\"\"Load the trained model from disk\"\"\"\n    global models, model_info\n    try:\n        if os.path.exists(MODEL_FILE) and os.path.exists(MODEL_INFO_FILE):\n            with open(MODEL_FILE, \"rb\") as f:\n                models = pickle.load(f)",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "data_lock",
        "kind": 5,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "data_lock = threading.Lock()\napp = Flask(__name__)\ndef load_model():\n    \"\"\"Load the trained model from disk\"\"\"\n    global models, model_info\n    try:\n        if os.path.exists(MODEL_FILE) and os.path.exists(MODEL_INFO_FILE):\n            with open(MODEL_FILE, \"rb\") as f:\n                models = pickle.load(f)\n            with open(MODEL_INFO_FILE, \"r\") as f:",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "models.serve",
        "description": "models.serve",
        "peekOfCode": "app = Flask(__name__)\ndef load_model():\n    \"\"\"Load the trained model from disk\"\"\"\n    global models, model_info\n    try:\n        if os.path.exists(MODEL_FILE) and os.path.exists(MODEL_INFO_FILE):\n            with open(MODEL_FILE, \"rb\") as f:\n                models = pickle.load(f)\n            with open(MODEL_INFO_FILE, \"r\") as f:\n                model_info = json.load(f)",
        "detail": "models.serve",
        "documentation": {}
    },
    {
        "label": "find_latest_data_files",
        "kind": 2,
        "importPath": "models.train",
        "description": "models.train",
        "peekOfCode": "def find_latest_data_files():\n    \"\"\"Find the latest data files in the data directory\"\"\"\n    data_dir = \"data\"\n    trends_file = None\n    driver_file = None\n    funnel_file = None\n    # Look for the most recent files\n    for file in os.listdir(data_dir):\n        if file.endswith(\".json\"):\n            file_path = os.path.join(data_dir, file)",
        "detail": "models.train",
        "documentation": {}
    },
    {
        "label": "train_and_save_model",
        "kind": 2,
        "importPath": "models.train",
        "description": "models.train",
        "peekOfCode": "def train_and_save_model():\n    \"\"\"Train the model and save it to disk\"\"\"\n    try:\n        logger.info(\"Starting model training\")\n        # Find the latest data files\n        trends_file, driver_file, funnel_file = find_latest_data_files()\n        if not trends_file or not driver_file:\n            logger.error(\"Missing required data files\")\n            return False\n        logger.info(f\"Using data files: {trends_file}, {driver_file}, {funnel_file}\")",
        "detail": "models.train",
        "documentation": {}
    },
    {
        "label": "scheduled_training",
        "kind": 2,
        "importPath": "models.train",
        "description": "models.train",
        "peekOfCode": "def scheduled_training():\n    \"\"\"Function to be called by the scheduler\"\"\"\n    logger.info(\"Running scheduled model training\")\n    success = train_and_save_model()\n    if success:\n        logger.info(\"Scheduled training completed successfully\")\n    else:\n        logger.error(\"Scheduled training failed\")\ndef main():\n    \"\"\"Main function to run the continuous training process\"\"\"",
        "detail": "models.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "models.train",
        "description": "models.train",
        "peekOfCode": "def main():\n    \"\"\"Main function to run the continuous training process\"\"\"\n    logger.info(\"Starting training service\")\n    # Train the model immediately on startup\n    train_and_save_model()\n    # Schedule training every 2 minutes\n    schedule.every(2).minutes.do(scheduled_training)\n    logger.info(\"Training service started, will train model every 2 minutes\")\n    # Run the scheduling loop\n    while True:",
        "detail": "models.train",
        "documentation": {}
    },
    {
        "label": "detailed_logger",
        "kind": 5,
        "importPath": "models.train",
        "description": "models.train",
        "peekOfCode": "detailed_logger = logging.getLogger(\"train_detailed\")\ndetailed_logger.setLevel(logging.INFO)\ndetailed_handler = logging.FileHandler(\"logs/train_output.log\", mode=\"a\")\ndetailed_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\"))\ndetailed_logger.addHandler(detailed_handler)\ndetailed_logger.propagate = False  # Prevent double logging\nlogger = logging.getLogger(\"train\")\n# Ensure the models directory exists\nif not os.path.exists(\"models\"):\n    os.makedirs(\"models\")",
        "detail": "models.train",
        "documentation": {}
    },
    {
        "label": "detailed_handler",
        "kind": 5,
        "importPath": "models.train",
        "description": "models.train",
        "peekOfCode": "detailed_handler = logging.FileHandler(\"logs/train_output.log\", mode=\"a\")\ndetailed_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\"))\ndetailed_logger.addHandler(detailed_handler)\ndetailed_logger.propagate = False  # Prevent double logging\nlogger = logging.getLogger(\"train\")\n# Ensure the models directory exists\nif not os.path.exists(\"models\"):\n    os.makedirs(\"models\")\n# Global variables\nMODEL_FILE = \"models/demand_model.pkl\"",
        "detail": "models.train",
        "documentation": {}
    },
    {
        "label": "detailed_logger.propagate",
        "kind": 5,
        "importPath": "models.train",
        "description": "models.train",
        "peekOfCode": "detailed_logger.propagate = False  # Prevent double logging\nlogger = logging.getLogger(\"train\")\n# Ensure the models directory exists\nif not os.path.exists(\"models\"):\n    os.makedirs(\"models\")\n# Global variables\nMODEL_FILE = \"models/demand_model.pkl\"\nFEATURES_FILE = \"models/model_features.pkl\"\nMODEL_INFO_FILE = \"models/model_info.json\"\ndef find_latest_data_files():",
        "detail": "models.train",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "models.train",
        "description": "models.train",
        "peekOfCode": "logger = logging.getLogger(\"train\")\n# Ensure the models directory exists\nif not os.path.exists(\"models\"):\n    os.makedirs(\"models\")\n# Global variables\nMODEL_FILE = \"models/demand_model.pkl\"\nFEATURES_FILE = \"models/model_features.pkl\"\nMODEL_INFO_FILE = \"models/model_info.json\"\ndef find_latest_data_files():\n    \"\"\"Find the latest data files in the data directory\"\"\"",
        "detail": "models.train",
        "documentation": {}
    },
    {
        "label": "MODEL_FILE",
        "kind": 5,
        "importPath": "models.train",
        "description": "models.train",
        "peekOfCode": "MODEL_FILE = \"models/demand_model.pkl\"\nFEATURES_FILE = \"models/model_features.pkl\"\nMODEL_INFO_FILE = \"models/model_info.json\"\ndef find_latest_data_files():\n    \"\"\"Find the latest data files in the data directory\"\"\"\n    data_dir = \"data\"\n    trends_file = None\n    driver_file = None\n    funnel_file = None\n    # Look for the most recent files",
        "detail": "models.train",
        "documentation": {}
    },
    {
        "label": "FEATURES_FILE",
        "kind": 5,
        "importPath": "models.train",
        "description": "models.train",
        "peekOfCode": "FEATURES_FILE = \"models/model_features.pkl\"\nMODEL_INFO_FILE = \"models/model_info.json\"\ndef find_latest_data_files():\n    \"\"\"Find the latest data files in the data directory\"\"\"\n    data_dir = \"data\"\n    trends_file = None\n    driver_file = None\n    funnel_file = None\n    # Look for the most recent files\n    for file in os.listdir(data_dir):",
        "detail": "models.train",
        "documentation": {}
    },
    {
        "label": "MODEL_INFO_FILE",
        "kind": 5,
        "importPath": "models.train",
        "description": "models.train",
        "peekOfCode": "MODEL_INFO_FILE = \"models/model_info.json\"\ndef find_latest_data_files():\n    \"\"\"Find the latest data files in the data directory\"\"\"\n    data_dir = \"data\"\n    trends_file = None\n    driver_file = None\n    funnel_file = None\n    # Look for the most recent files\n    for file in os.listdir(data_dir):\n        if file.endswith(\".json\"):",
        "detail": "models.train",
        "documentation": {}
    },
    {
        "label": "timed_cache",
        "kind": 2,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "def timed_cache(timeout_seconds=300):\n    def decorator(func):\n        cache = {}\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = str(args) + str(kwargs)\n            current_time = datetime.now()\n            with cache_lock:\n                # Check if result is in cache and not expired\n                if key in cache:",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "get_cache_file_path",
        "kind": 2,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "def get_cache_file_path(cache_key):\n    \"\"\"Generate a sanitized file path for the cache key\"\"\"\n    # Replace characters that aren't valid in filenames\n    safe_key = re.sub(r\"[^\\w\\-_]\", \"_\", cache_key)\n    return os.path.join(CACHE_DIR, f\"{safe_key}.json\")\ndef load_from_file_cache(cache_key, max_age_seconds=1800):\n    \"\"\"Load data from file cache if it exists and is not expired\"\"\"\n    file_path = get_cache_file_path(cache_key)\n    if os.path.exists(file_path):\n        try:",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "load_from_file_cache",
        "kind": 2,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "def load_from_file_cache(cache_key, max_age_seconds=1800):\n    \"\"\"Load data from file cache if it exists and is not expired\"\"\"\n    file_path = get_cache_file_path(cache_key)\n    if os.path.exists(file_path):\n        try:\n            with open(file_path, \"r\") as f:\n                cache_data = json.load(f)\n            # Check if the cache is still valid\n            cache_time = datetime.fromisoformat(\n                cache_data.get(\"cached_at\", \"2000-01-01T00:00:00\")",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "save_to_file_cache",
        "kind": 2,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "def save_to_file_cache(cache_key, data):\n    \"\"\"Save data to file cache\"\"\"\n    file_path = get_cache_file_path(cache_key)\n    try:\n        cache_entry = {\n            \"data\": data,\n            \"cached_at\": datetime.now().isoformat(),\n            \"key\": cache_key,\n        }\n        with open(file_path, \"w\") as f:",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "logger = logging.getLogger(\"forecast_api\")\n# Create Flask app\napp = FastAPI()\n# Enable CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"GET\", \"POST\"],\n    allow_headers=[\"*\"],",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "app = FastAPI()\n# Enable CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"GET\", \"POST\"],\n    allow_headers=[\"*\"],\n)\n# Initialize data manager and forecaster",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "data_manager",
        "kind": 5,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "data_manager = RideDataManager()\nforecaster = RideRequestForecast(data_manager)\n# Cache for forecast results\nforecast_cache = {}\n# Lock for thread safety\ncache_lock = threading.Lock()\n# Ensure cache directory exists\nCACHE_DIR = \"cache/forecasts\"\nos.makedirs(CACHE_DIR, exist_ok=True)\nLOGS_DIR = \"logs\"",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "forecaster",
        "kind": 5,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "forecaster = RideRequestForecast(data_manager)\n# Cache for forecast results\nforecast_cache = {}\n# Lock for thread safety\ncache_lock = threading.Lock()\n# Ensure cache directory exists\nCACHE_DIR = \"cache/forecasts\"\nos.makedirs(CACHE_DIR, exist_ok=True)\nLOGS_DIR = \"logs\"\nos.makedirs(LOGS_DIR, exist_ok=True)",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "forecast_cache",
        "kind": 5,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "forecast_cache = {}\n# Lock for thread safety\ncache_lock = threading.Lock()\n# Ensure cache directory exists\nCACHE_DIR = \"cache/forecasts\"\nos.makedirs(CACHE_DIR, exist_ok=True)\nLOGS_DIR = \"logs\"\nos.makedirs(LOGS_DIR, exist_ok=True)\n# Load historical data and models, and fetch current data\ntry:",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "cache_lock",
        "kind": 5,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "cache_lock = threading.Lock()\n# Ensure cache directory exists\nCACHE_DIR = \"cache/forecasts\"\nos.makedirs(CACHE_DIR, exist_ok=True)\nLOGS_DIR = \"logs\"\nos.makedirs(LOGS_DIR, exist_ok=True)\n# Load historical data and models, and fetch current data\ntry:\n    logger.info(\"Loading historical data and models...\")\n    data_manager.load_historical_data()",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "CACHE_DIR",
        "kind": 5,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "CACHE_DIR = \"cache/forecasts\"\nos.makedirs(CACHE_DIR, exist_ok=True)\nLOGS_DIR = \"logs\"\nos.makedirs(LOGS_DIR, exist_ok=True)\n# Load historical data and models, and fetch current data\ntry:\n    logger.info(\"Loading historical data and models...\")\n    data_manager.load_historical_data()\n    logger.info(\"Fetching current data from endpoints...\")\n    fetched_data = data_manager.fetch_all_endpoints()",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "LOGS_DIR",
        "kind": 5,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "LOGS_DIR = \"logs\"\nos.makedirs(LOGS_DIR, exist_ok=True)\n# Load historical data and models, and fetch current data\ntry:\n    logger.info(\"Loading historical data and models...\")\n    data_manager.load_historical_data()\n    logger.info(\"Fetching current data from endpoints...\")\n    fetched_data = data_manager.fetch_all_endpoints()\n    if fetched_data:\n        logger.info(\"Successfully fetched current data\")",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "MINIMUM_PRICE",
        "kind": 5,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "MINIMUM_PRICE = 30  # 30 minimum starting fare\nPER_KM_VALUE = 15  # 15 per kilometer\nPER_MIN_CHARGE = 1.5  # 1.5 per minute in traffic\n# Cache decorator with timeout\ndef timed_cache(timeout_seconds=300):\n    def decorator(func):\n        cache = {}\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = str(args) + str(kwargs)",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "PER_KM_VALUE",
        "kind": 5,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "PER_KM_VALUE = 15  # 15 per kilometer\nPER_MIN_CHARGE = 1.5  # 1.5 per minute in traffic\n# Cache decorator with timeout\ndef timed_cache(timeout_seconds=300):\n    def decorator(func):\n        cache = {}\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = str(args) + str(kwargs)\n            current_time = datetime.now()",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "PER_MIN_CHARGE",
        "kind": 5,
        "importPath": "api",
        "description": "api",
        "peekOfCode": "PER_MIN_CHARGE = 1.5  # 1.5 per minute in traffic\n# Cache decorator with timeout\ndef timed_cache(timeout_seconds=300):\n    def decorator(func):\n        cache = {}\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = str(args) + str(kwargs)\n            current_time = datetime.now()\n            with cache_lock:",
        "detail": "api",
        "documentation": {}
    },
    {
        "label": "parse_iso_date",
        "kind": 2,
        "importPath": "meh",
        "description": "meh",
        "peekOfCode": "def parse_iso_date(date_string):\n    \"\"\"Parse ISO format dates with Z timezone indicator\"\"\"\n    if date_string.endswith('Z'):\n        date_string = date_string[:-1] + '+00:00'\n    return datetime.fromisoformat(date_string)\n# Cache decorator with timeout\ndef timed_cache(timeout_seconds=300):\n    def decorator(func):\n        cache = {}\n        @functools.wraps(func)",
        "detail": "meh",
        "documentation": {}
    },
    {
        "label": "timed_cache",
        "kind": 2,
        "importPath": "meh",
        "description": "meh",
        "peekOfCode": "def timed_cache(timeout_seconds=300):\n    def decorator(func):\n        cache = {}\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = str(args) + str(kwargs)\n            current_time = datetime.now()\n            with cache_lock:\n                # Check if result is in cache and not expired\n                if key in cache:",
        "detail": "meh",
        "documentation": {}
    },
    {
        "label": "get_cache_file_path",
        "kind": 2,
        "importPath": "meh",
        "description": "meh",
        "peekOfCode": "def get_cache_file_path(cache_key):\n    \"\"\"Generate a sanitized file path for the cache key\"\"\"\n    # Replace characters that aren't valid in filenames\n    safe_key = re.sub(r\"[^\\w\\-_]\", \"_\", cache_key)\n    return os.path.join(CACHE_DIR, f\"{safe_key}.json\")\ndef load_from_file_cache(cache_key, max_age_seconds=1800):\n    \"\"\"Load data from file cache if it exists and is not expired\"\"\"\n    file_path = get_cache_file_path(cache_key)\n    if os.path.exists(file_path):\n        try:",
        "detail": "meh",
        "documentation": {}
    },
    {
        "label": "load_from_file_cache",
        "kind": 2,
        "importPath": "meh",
        "description": "meh",
        "peekOfCode": "def load_from_file_cache(cache_key, max_age_seconds=1800):\n    \"\"\"Load data from file cache if it exists and is not expired\"\"\"\n    file_path = get_cache_file_path(cache_key)\n    if os.path.exists(file_path):\n        try:\n            with open(file_path, \"r\") as f:\n                cache_data = json.load(f)\n            # Check if the cache is still valid\n            cache_time = datetime.fromisoformat(\n                cache_data.get(\"cached_at\", \"2000-01-01T00:00:00\")",
        "detail": "meh",
        "documentation": {}
    },
    {
        "label": "save_to_file_cache",
        "kind": 2,
        "importPath": "meh",
        "description": "meh",
        "peekOfCode": "def save_to_file_cache(cache_key, data):\n    \"\"\"Save data to file cache\"\"\"\n    file_path = get_cache_file_path(cache_key)\n    try:\n        cache_entry = {\n            \"data\": data,\n            \"cached_at\": datetime.now().isoformat(),\n            \"key\": cache_key,\n        }\n        with open(file_path, \"w\") as f:",
        "detail": "meh",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "meh",
        "description": "meh",
        "peekOfCode": "logger = logging.getLogger(\"forecast_api\")\napp = FastAPI()\n# Enable CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"GET\", \"POST\"],\n    allow_headers=[\"*\"],\n)",
        "detail": "meh",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "meh",
        "description": "meh",
        "peekOfCode": "app = FastAPI()\n# Enable CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"GET\", \"POST\"],\n    allow_headers=[\"*\"],\n)\n# Initialize data manager and forecaster",
        "detail": "meh",
        "documentation": {}
    },
    {
        "label": "data_manager",
        "kind": 5,
        "importPath": "meh",
        "description": "meh",
        "peekOfCode": "data_manager = RideDataManager()\nforecaster = RideRequestForecast(data_manager)\n# Cache for forecast results\nforecast_cache = {}\n# Lock for thread safety\ncache_lock = threading.Lock()\n# Ensure cache directory exists\nCACHE_DIR = \"cache/forecasts\"\nos.makedirs(CACHE_DIR, exist_ok=True)\n# Load historical data and models",
        "detail": "meh",
        "documentation": {}
    },
    {
        "label": "forecaster",
        "kind": 5,
        "importPath": "meh",
        "description": "meh",
        "peekOfCode": "forecaster = RideRequestForecast(data_manager)\n# Cache for forecast results\nforecast_cache = {}\n# Lock for thread safety\ncache_lock = threading.Lock()\n# Ensure cache directory exists\nCACHE_DIR = \"cache/forecasts\"\nos.makedirs(CACHE_DIR, exist_ok=True)\n# Load historical data and models\ntry:",
        "detail": "meh",
        "documentation": {}
    },
    {
        "label": "forecast_cache",
        "kind": 5,
        "importPath": "meh",
        "description": "meh",
        "peekOfCode": "forecast_cache = {}\n# Lock for thread safety\ncache_lock = threading.Lock()\n# Ensure cache directory exists\nCACHE_DIR = \"cache/forecasts\"\nos.makedirs(CACHE_DIR, exist_ok=True)\n# Load historical data and models\ntry:\n    data_manager.load_historical_data()\n    forecaster.load_model(\"ward\")",
        "detail": "meh",
        "documentation": {}
    },
    {
        "label": "cache_lock",
        "kind": 5,
        "importPath": "meh",
        "description": "meh",
        "peekOfCode": "cache_lock = threading.Lock()\n# Ensure cache directory exists\nCACHE_DIR = \"cache/forecasts\"\nos.makedirs(CACHE_DIR, exist_ok=True)\n# Load historical data and models\ntry:\n    data_manager.load_historical_data()\n    forecaster.load_model(\"ward\")\n    logger.info(\"Historical data and models loaded successfully\")\nexcept Exception as e:",
        "detail": "meh",
        "documentation": {}
    },
    {
        "label": "CACHE_DIR",
        "kind": 5,
        "importPath": "meh",
        "description": "meh",
        "peekOfCode": "CACHE_DIR = \"cache/forecasts\"\nos.makedirs(CACHE_DIR, exist_ok=True)\n# Load historical data and models\ntry:\n    data_manager.load_historical_data()\n    forecaster.load_model(\"ward\")\n    logger.info(\"Historical data and models loaded successfully\")\nexcept Exception as e:\n    logger.error(f\"Error loading data or models: {e}\")\n    logger.error(traceback.format_exc())",
        "detail": "meh",
        "documentation": {}
    },
    {
        "label": "MINIMUM_PRICE",
        "kind": 5,
        "importPath": "meh",
        "description": "meh",
        "peekOfCode": "MINIMUM_PRICE = 30  # 30 minimum starting fare\nPER_KM_VALUE = 15  # 15 per kilometer\nPER_MIN_CHARGE = 1.5  # 1.5 per minute in traffic\ndef parse_iso_date(date_string):\n    \"\"\"Parse ISO format dates with Z timezone indicator\"\"\"\n    if date_string.endswith('Z'):\n        date_string = date_string[:-1] + '+00:00'\n    return datetime.fromisoformat(date_string)\n# Cache decorator with timeout\ndef timed_cache(timeout_seconds=300):",
        "detail": "meh",
        "documentation": {}
    },
    {
        "label": "PER_KM_VALUE",
        "kind": 5,
        "importPath": "meh",
        "description": "meh",
        "peekOfCode": "PER_KM_VALUE = 15  # 15 per kilometer\nPER_MIN_CHARGE = 1.5  # 1.5 per minute in traffic\ndef parse_iso_date(date_string):\n    \"\"\"Parse ISO format dates with Z timezone indicator\"\"\"\n    if date_string.endswith('Z'):\n        date_string = date_string[:-1] + '+00:00'\n    return datetime.fromisoformat(date_string)\n# Cache decorator with timeout\ndef timed_cache(timeout_seconds=300):\n    def decorator(func):",
        "detail": "meh",
        "documentation": {}
    },
    {
        "label": "PER_MIN_CHARGE",
        "kind": 5,
        "importPath": "meh",
        "description": "meh",
        "peekOfCode": "PER_MIN_CHARGE = 1.5  # 1.5 per minute in traffic\ndef parse_iso_date(date_string):\n    \"\"\"Parse ISO format dates with Z timezone indicator\"\"\"\n    if date_string.endswith('Z'):\n        date_string = date_string[:-1] + '+00:00'\n    return datetime.fromisoformat(date_string)\n# Cache decorator with timeout\ndef timed_cache(timeout_seconds=300):\n    def decorator(func):\n        cache = {}",
        "detail": "meh",
        "documentation": {}
    },
    {
        "label": "RideDataManager",
        "kind": 6,
        "importPath": "panda",
        "description": "panda",
        "peekOfCode": "class RideDataManager:\n    \"\"\"Class to manage ride data fetching, storage, and retrieval\"\"\"\n    def __init__(self):\n        self.last_fetch_time = None\n        self.historical_data = {}\n    def fetch_endpoint(self, endpoint):\n        \"\"\"Fetch data from a specific endpoint\"\"\"\n        url = f\"{BASE_URL}/{endpoint}\"\n        try:\n            logger.info(f\"Fetching data from {url}\")",
        "detail": "panda",
        "documentation": {}
    },
    {
        "label": "RideRequestForecast",
        "kind": 6,
        "importPath": "panda",
        "description": "panda",
        "peekOfCode": "class RideRequestForecast:\n    \"\"\"Class to forecast ride requests based on historical data\"\"\"\n    def __init__(self, data_manager):\n        self.data_manager = data_manager\n        self.model = None\n        self.features = []\n        self.last_training_time = None\n        self.forecast_history = []\n    def prepare_data_for_modeling(self, data_type=\"ward\"):\n        \"\"\"Prepare data for modeling\"\"\"",
        "detail": "panda",
        "documentation": {}
    },
    {
        "label": "ForecastService",
        "kind": 6,
        "importPath": "panda",
        "description": "panda",
        "peekOfCode": "class ForecastService:\n    \"\"\"Main service to run the forecasting process\"\"\"\n    def __init__(self):\n        self.data_manager = RideDataManager()\n        self.forecaster = RideRequestForecast(self.data_manager)\n        self.last_fetch_time = None\n        self.last_forecast_time = None\n    def initialize(self):\n        \"\"\"Initialize the service\"\"\"\n        try:",
        "detail": "panda",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "panda",
        "description": "panda",
        "peekOfCode": "def main():\n    \"\"\"Main function to run the forecast service\"\"\"\n    try:\n        # Create and run the service\n        service = ForecastService()\n        # Initialize and run the service\n        service.initialize()\n        service.generate_forecast()\n        service.run()\n        return 0",
        "detail": "panda",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "panda",
        "description": "panda",
        "peekOfCode": "logger = logging.getLogger(\"forecast\")\n# Create a separate logger for detailed output\ndetailed_logger = logging.getLogger(\"forecast_detailed\")\ndetailed_logger.setLevel(logging.INFO)\ndetailed_handler = logging.FileHandler(\"logs/forecast_output.log\", mode=\"a\")\ndetailed_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\"))\ndetailed_logger.addHandler(detailed_handler)\ndetailed_logger.propagate = False  # Prevent double logging\n# Ensure necessary directories exist\nos.makedirs(\"logs\", exist_ok=True)",
        "detail": "panda",
        "documentation": {}
    },
    {
        "label": "detailed_logger",
        "kind": 5,
        "importPath": "panda",
        "description": "panda",
        "peekOfCode": "detailed_logger = logging.getLogger(\"forecast_detailed\")\ndetailed_logger.setLevel(logging.INFO)\ndetailed_handler = logging.FileHandler(\"logs/forecast_output.log\", mode=\"a\")\ndetailed_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\"))\ndetailed_logger.addHandler(detailed_handler)\ndetailed_logger.propagate = False  # Prevent double logging\n# Ensure necessary directories exist\nos.makedirs(\"logs\", exist_ok=True)\nos.makedirs(\"data\", exist_ok=True)\nos.makedirs(\"data/historical\", exist_ok=True)",
        "detail": "panda",
        "documentation": {}
    },
    {
        "label": "detailed_handler",
        "kind": 5,
        "importPath": "panda",
        "description": "panda",
        "peekOfCode": "detailed_handler = logging.FileHandler(\"logs/forecast_output.log\", mode=\"a\")\ndetailed_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\"))\ndetailed_logger.addHandler(detailed_handler)\ndetailed_logger.propagate = False  # Prevent double logging\n# Ensure necessary directories exist\nos.makedirs(\"logs\", exist_ok=True)\nos.makedirs(\"data\", exist_ok=True)\nos.makedirs(\"data/historical\", exist_ok=True)\nos.makedirs(\"data/models\", exist_ok=True)\nos.makedirs(\"data/forecasts\", exist_ok=True)",
        "detail": "panda",
        "documentation": {}
    },
    {
        "label": "detailed_logger.propagate",
        "kind": 5,
        "importPath": "panda",
        "description": "panda",
        "peekOfCode": "detailed_logger.propagate = False  # Prevent double logging\n# Ensure necessary directories exist\nos.makedirs(\"logs\", exist_ok=True)\nos.makedirs(\"data\", exist_ok=True)\nos.makedirs(\"data/historical\", exist_ok=True)\nos.makedirs(\"data/models\", exist_ok=True)\nos.makedirs(\"data/forecasts\", exist_ok=True)\nos.makedirs(\"data/visualizations\", exist_ok=True)\n# Base URL for the CDN\nBASE_URL = \"https://d11gklsvr97l1g.cloudfront.net/open/json-data\"",
        "detail": "panda",
        "documentation": {}
    },
    {
        "label": "BASE_URL",
        "kind": 5,
        "importPath": "panda",
        "description": "panda",
        "peekOfCode": "BASE_URL = \"https://d11gklsvr97l1g.cloudfront.net/open/json-data\"\n# List of endpoints to fetch\nENDPOINTS = [\n    \"trends_live_ward_new_key.json\",\n    \"trends_live_ca_new_key.json\",\n    \"driver_eda_wards_new_key.json\",\n    \"driver_eda_ca_new_key.json\",\n    \"funnel_live_ward_new_key.json\",\n    \"funnel_live_ca_new_key.json\",\n    \"funnel_cumulative_ward_new_key.json\",",
        "detail": "panda",
        "documentation": {}
    },
    {
        "label": "ENDPOINTS",
        "kind": 5,
        "importPath": "panda",
        "description": "panda",
        "peekOfCode": "ENDPOINTS = [\n    \"trends_live_ward_new_key.json\",\n    \"trends_live_ca_new_key.json\",\n    \"driver_eda_wards_new_key.json\",\n    \"driver_eda_ca_new_key.json\",\n    \"funnel_live_ward_new_key.json\",\n    \"funnel_live_ca_new_key.json\",\n    \"funnel_cumulative_ward_new_key.json\",\n    \"funnel_cumulative_ca_new_key.json\",\n    \"cumulative_stats_new_key.json\",",
        "detail": "panda",
        "documentation": {}
    },
    {
        "label": "print_success",
        "kind": 2,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "def print_success(message):\n    print(f\"{GREEN} {message}{RESET}\")\ndef print_error(message):\n    print(f\"{RED} {message}{RESET}\")\ndef print_warning(message):\n    print(f\"{YELLOW}! {message}{RESET}\")\ndef print_info(message):\n    print(f\"{BLUE} {message}{RESET}\")\ndef test_health(base_url):\n    \"\"\"Test the health endpoint\"\"\"",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "print_error",
        "kind": 2,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "def print_error(message):\n    print(f\"{RED} {message}{RESET}\")\ndef print_warning(message):\n    print(f\"{YELLOW}! {message}{RESET}\")\ndef print_info(message):\n    print(f\"{BLUE} {message}{RESET}\")\ndef test_health(base_url):\n    \"\"\"Test the health endpoint\"\"\"\n    try:\n        start_time = time.time()",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "print_warning",
        "kind": 2,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "def print_warning(message):\n    print(f\"{YELLOW}! {message}{RESET}\")\ndef print_info(message):\n    print(f\"{BLUE} {message}{RESET}\")\ndef test_health(base_url):\n    \"\"\"Test the health endpoint\"\"\"\n    try:\n        start_time = time.time()\n        response = requests.get(f\"{base_url}/health\", timeout=5)\n        elapsed = time.time() - start_time",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "print_info",
        "kind": 2,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "def print_info(message):\n    print(f\"{BLUE} {message}{RESET}\")\ndef test_health(base_url):\n    \"\"\"Test the health endpoint\"\"\"\n    try:\n        start_time = time.time()\n        response = requests.get(f\"{base_url}/health\", timeout=5)\n        elapsed = time.time() - start_time\n        if response.status_code == 200:\n            data = response.json()",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "test_health",
        "kind": 2,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "def test_health(base_url):\n    \"\"\"Test the health endpoint\"\"\"\n    try:\n        start_time = time.time()\n        response = requests.get(f\"{base_url}/health\", timeout=5)\n        elapsed = time.time() - start_time\n        if response.status_code == 200:\n            data = response.json()\n            print_success(f\"Health check passed in {elapsed:.2f}s: {data}\")\n            return True",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "test_regions",
        "kind": 2,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "def test_regions(base_url, data_type=\"ward\"):\n    \"\"\"Test the regions endpoint with different filters\"\"\"\n    try:\n        # Test with no filter (all regions)\n        start_time = time.time()\n        response = requests.get(f\"{base_url}/regions?type={data_type}\", timeout=10)\n        elapsed = time.time() - start_time\n        if response.status_code == 200:\n            data = response.json()\n            all_regions = data.get(\"regions\", [])",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "test_historical",
        "kind": 2,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "def test_historical(base_url, data_type=\"ward\", hours=24, region=\"1\"):\n    \"\"\"Test the historical endpoint\"\"\"\n    try:\n        start_time = time.time()\n        response = requests.get(\n            f\"{base_url}/historical?type={data_type}&hours={hours}&region={region}\",\n            timeout=20,\n        )\n        elapsed = time.time() - start_time\n        if response.status_code == 200:",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "test_forecast",
        "kind": 2,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "def test_forecast(base_url, data_type=\"ward\", hours=12, region=\"1\"):\n    \"\"\"Test the forecast endpoint\"\"\"\n    try:\n        print_info(\n            f\"Testing forecast for region {region}, this may take up to 60 seconds...\"\n        )\n        start_time = time.time()\n        response = requests.get(\n            f\"{base_url}/forecast?type={data_type}&hours={hours}&region={region}\",\n            timeout=60,",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "test_model_info",
        "kind": 2,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "def test_model_info(base_url, data_type=\"ward\"):\n    \"\"\"Test the model info endpoint\"\"\"\n    try:\n        start_time = time.time()\n        response = requests.get(f\"{base_url}/model/info?type={data_type}\", timeout=10)\n        elapsed = time.time() - start_time\n        if response.status_code == 200:\n            data = response.json()\n            print_success(\n                f\"Model info endpoint returned data in {elapsed:.2f}s: {data}\"",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "test_cache_clear",
        "kind": 2,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "def test_cache_clear(base_url):\n    \"\"\"Test the cache clear endpoint\"\"\"\n    try:\n        start_time = time.time()\n        response = requests.post(f\"{base_url}/cache/clear\", timeout=10)\n        elapsed = time.time() - start_time\n        if response.status_code == 200:\n            data = response.json()\n            print_success(f\"Cache clear endpoint succeeded in {elapsed:.2f}s: {data}\")\n            return True",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "test_string_region_forecast",
        "kind": 2,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "def test_string_region_forecast(base_url, data_type=\"ward\", hours=12):\n    \"\"\"Test the forecast endpoint with string-based region identifiers\"\"\"\n    # Test with a b_ prefixed region\n    b_region_success = False\n    try:\n        print_info(\"Testing forecast with b_ prefixed region...\")\n        response = requests.get(\n            f\"{base_url}/regions?type={data_type}&filter=b\", timeout=10\n        )\n        if response.status_code == 200:",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "test_forecast_all",
        "kind": 2,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "def test_forecast_all(\n    base_url, data_type=\"ward\", hours=12, filter_type=\"all\", workers=4\n):\n    \"\"\"Test the forecast/all endpoint that returns forecasts for all regions\"\"\"\n    try:\n        print_info(\n            f\"Testing forecast/all endpoint with filter={filter_type}, workers={workers}, this may take up to 90 seconds...\"\n        )\n        start_time = time.time()\n        response = requests.get(",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=\"Test the NoCapRide API endpoints\")\n    parser.add_argument(\"--url\", default=API_BASE_URL, help=\"Base URL for the API\")\n    parser.add_argument(\n        \"--type\",\n        default=\"ward\",\n        choices=[\"ward\", \"ca\"],\n        help=\"Data type to use for testing\",\n    )\n    parser.add_argument(\"--region\", default=\"1\", help=\"Region to use for testing\")",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "API_BASE_URL",
        "kind": 5,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "API_BASE_URL = \"http://localhost:8888/api\"\n# Colors for terminal output\nGREEN = \"\\033[92m\"\nRED = \"\\033[91m\"\nYELLOW = \"\\033[93m\"\nBLUE = \"\\033[94m\"\nRESET = \"\\033[0m\"\ndef print_success(message):\n    print(f\"{GREEN} {message}{RESET}\")\ndef print_error(message):",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "GREEN",
        "kind": 5,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "GREEN = \"\\033[92m\"\nRED = \"\\033[91m\"\nYELLOW = \"\\033[93m\"\nBLUE = \"\\033[94m\"\nRESET = \"\\033[0m\"\ndef print_success(message):\n    print(f\"{GREEN} {message}{RESET}\")\ndef print_error(message):\n    print(f\"{RED} {message}{RESET}\")\ndef print_warning(message):",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "RED",
        "kind": 5,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "RED = \"\\033[91m\"\nYELLOW = \"\\033[93m\"\nBLUE = \"\\033[94m\"\nRESET = \"\\033[0m\"\ndef print_success(message):\n    print(f\"{GREEN} {message}{RESET}\")\ndef print_error(message):\n    print(f\"{RED} {message}{RESET}\")\ndef print_warning(message):\n    print(f\"{YELLOW}! {message}{RESET}\")",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "YELLOW",
        "kind": 5,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "YELLOW = \"\\033[93m\"\nBLUE = \"\\033[94m\"\nRESET = \"\\033[0m\"\ndef print_success(message):\n    print(f\"{GREEN} {message}{RESET}\")\ndef print_error(message):\n    print(f\"{RED} {message}{RESET}\")\ndef print_warning(message):\n    print(f\"{YELLOW}! {message}{RESET}\")\ndef print_info(message):",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "BLUE",
        "kind": 5,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "BLUE = \"\\033[94m\"\nRESET = \"\\033[0m\"\ndef print_success(message):\n    print(f\"{GREEN} {message}{RESET}\")\ndef print_error(message):\n    print(f\"{RED} {message}{RESET}\")\ndef print_warning(message):\n    print(f\"{YELLOW}! {message}{RESET}\")\ndef print_info(message):\n    print(f\"{BLUE} {message}{RESET}\")",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "RESET",
        "kind": 5,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "RESET = \"\\033[0m\"\ndef print_success(message):\n    print(f\"{GREEN} {message}{RESET}\")\ndef print_error(message):\n    print(f\"{RED} {message}{RESET}\")\ndef print_warning(message):\n    print(f\"{YELLOW}! {message}{RESET}\")\ndef print_info(message):\n    print(f\"{BLUE} {message}{RESET}\")\ndef test_health(base_url):",
        "detail": "test_api",
        "documentation": {}
    }
]